{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP SQL Agent Demo\n",
    "\n",
    "This notebook demonstrates how to use the Model Context Protocol (MCP) with LangChain to create an agent that can query multiple data sources:\n",
    "\n",
    "- PostgreSQL (relational database)\n",
    "- Snowflake (data warehouse)\n",
    "- Grafana (metrics and dashboards)\n",
    "\n",
    "The agent will be able to select the appropriate data source based on the user's question and use it to provide answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import shared environment utilities\n",
    "repo_root = Path.cwd().parent.parent.parent.parent\n",
    "sys.path.append(str(repo_root))\n",
    "from shared.utils.env import load_env\n",
    "\n",
    "# Load environment variables\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import LangChain components\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.tools import BaseTool, StructuredTool\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "# Configure the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    temperature=0,\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MCP Servers\n",
    "\n",
    "Now, let's set up connections to our MCP servers for each data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from mcp.client import MCPClient\n",
    "\n",
    "# Connect to PostgreSQL MCP server\n",
    "postgres_mcp = MCPClient(\"http://mcp-postgres:8080\")\n",
    "\n",
    "# Connect to Snowflake MCP server\n",
    "snowflake_mcp = MCPClient(\"http://mcp-snowflake:8080\")\n",
    "\n",
    "# Connect to Grafana MCP server\n",
    "grafana_mcp = MCPClient(\"http://mcp-grafana:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tools for LangChain Agent\n",
    "\n",
    "Let's create tools for each data source that our agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# PostgreSQL query tool\n",
    "def query_postgres(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against PostgreSQL and return the results.\"\"\"\n",
    "    try:\n",
    "        response = postgres_mcp.query(query)\n",
    "        df = pd.DataFrame(response['rows'], columns=response['columns'])\n",
    "        return df.to_markdown()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying PostgreSQL: {str(e)}\"\n",
    "\n",
    "postgres_tool = StructuredTool.from_function(\n",
    "    func=query_postgres,\n",
    "    name=\"query_postgres\",\n",
    "    description=\"Execute SQL queries against PostgreSQL database with e-commerce data (users, products, orders)\"\n",
    ")\n",
    "\n",
    "# PostgreSQL schema info tool\n",
    "def get_postgres_schema() -> str:\n",
    "    \"\"\"Get the schema information for PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        schema_query = \"\"\"\n",
    "        SELECT \n",
    "            table_name, \n",
    "            column_name, \n",
    "            data_type, \n",
    "            is_nullable \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_schema = 'public' \n",
    "        ORDER BY table_name, ordinal_position;\n",
    "        \"\"\"\n",
    "        response = postgres_mcp.query(schema_query)\n",
    "        df = pd.DataFrame(response['rows'], columns=response['columns'])\n",
    "        return df.to_markdown()\n",
    "    except Exception as e:\n",
    "        return f\"Error getting PostgreSQL schema: {str(e)}\"\n",
    "\n",
    "postgres_schema_tool = StructuredTool.from_function(\n",
    "    func=get_postgres_schema,\n",
    "    name=\"get_postgres_schema\",\n",
    "    description=\"Get schema information for PostgreSQL database tables and columns\"\n",
    ")\n",
    "\n",
    "# Snowflake query tool\n",
    "def query_snowflake(query: str) -> str:\n",
    "    \"\"\"Execute a SQL query against Snowflake and return the results.\"\"\"\n",
    "    try:\n",
    "        response = snowflake_mcp.query(query)\n",
    "        df = pd.DataFrame(response['rows'], columns=response['columns'])\n",
    "        return df.to_markdown()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Snowflake: {str(e)}\"\n",
    "\n",
    "snowflake_tool = StructuredTool.from_function(\n",
    "    func=query_snowflake,\n",
    "    name=\"query_snowflake\",\n",
    "    description=\"Execute SQL queries against Snowflake data warehouse\"\n",
    ")\n",
    "\n",
    "# Grafana metrics query tool\n",
    "def query_grafana_metrics(datasource: str, query: str, start: str, end: str, step: int = 60) -> str:\n",
    "    \"\"\"Query Grafana for metrics data.\"\"\"\n",
    "    try:\n",
    "        response = grafana_mcp.query_metrics(datasource=datasource, query=query, start=start, end=end, step=step)\n",
    "        return json.dumps(response, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grafana metrics: {str(e)}\"\n",
    "\n",
    "grafana_metrics_tool = StructuredTool.from_function(\n",
    "    func=query_grafana_metrics,\n",
    "    name=\"query_grafana_metrics\",\n",
    "    description=\"Query Grafana for metrics data using PromQL or Flux queries\"\n",
    ")\n",
    "\n",
    "# Grafana dashboards list tool\n",
    "def get_grafana_dashboards() -> str:\n",
    "    \"\"\"Get a list of available Grafana dashboards.\"\"\"\n",
    "    try:\n",
    "        response = grafana_mcp.get_dashboards()\n",
    "        return json.dumps(response, indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error getting Grafana dashboards: {str(e)}\"\n",
    "\n",
    "grafana_dashboards_tool = StructuredTool.from_function(\n",
    "    func=get_grafana_dashboards,\n",
    "    name=\"get_grafana_dashboards\",\n",
    "    description=\"Get a list of available Grafana dashboards\"\n",
    ")\n",
    "\n",
    "# Combine all tools\n",
    "tools = [\n",
    "    postgres_tool,\n",
    "    postgres_schema_tool,\n",
    "    snowflake_tool,\n",
    "    grafana_metrics_tool,\n",
    "    grafana_dashboards_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent\n",
    "\n",
    "Now, let's create our SQL agent using LangChain and the tools we've defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the prompt template for our agent\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert SQL and data analytics agent that can query multiple data sources including PostgreSQL, Snowflake, and Grafana.\n",
    "\n",
    "PostgreSQL contains e-commerce data with tables for users, products, orders, and order_items.\n",
    "\n",
    "Always first check the schema of the database before running queries to understand the available tables and columns.\n",
    "\n",
    "When analyzing data, provide insights and explanations about the results, not just raw data.\n",
    "\n",
    "For time-series data from Grafana, consider adding visualizations where appropriate.\n",
    "\n",
    "If you're unsure which data source to use, ask for clarification.\n",
    "\n",
    "User's question: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's test our agent with some sample questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test with a PostgreSQL query\n",
    "response = agent_executor.invoke({\"input\": \"What are the top 3 products by sales?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test with a question about order statistics\n",
    "response = agent_executor.invoke({\"input\": \"What's the average order value and how many orders do we have per status?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try More Complex Queries\n",
    "\n",
    "Now let's try some more complex scenarios that might require data from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ask a question that might require multiple data sources\n",
    "response = agent_executor.invoke({\"input\": \"Can you analyze the relationship between server metrics and sales performance over the last week?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Interactive mode - ask your own questions\n",
    "def ask_agent(question):\n",
    "    response = agent_executor.invoke({\"input\": question})\n",
    "    return response[\"output\"]\n",
    "\n",
    "# Example usage\n",
    "question = \"Which users have spent the most money, and what categories of products are they buying?\"\n",
    "answer = ask_agent(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the Model Context Protocol (MCP) to create an agent that can query multiple data sources. The agent can analyze data from PostgreSQL, Snowflake, and Grafana, providing a unified interface for data exploration and analysis.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. MCP provides a simple way to connect LLMs to various data sources\n",
    "2. LangChain makes it easy to create agents that can use multiple tools\n",
    "3. This approach allows for flexible, natural language interaction with your data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}